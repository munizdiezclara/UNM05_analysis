---
title: "UNM05_further_analysis"
output: pdf_document
date: "2023-07-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(afex)
library(BayesFactor)
library(apa)
library(emmeans)
library("writexl")
load("C:/Users/Clara/OneDrive - Lancaster University/Experiments/UNM05_analysis/UNM05_proc_data.RData")
```
#Data preparation(high/low accuracy)

Let´s re-do the analysis for high and low accuracy.
```{r}
#calculate and plot individual accuracy and maintain session
part_mean_train_acc <- training[complete.cases(training$correct_answer), ] %>%
  group_by(pNum, session) %>%
  summarize(acc = mean(correct_answer, na.rm = TRUE), 
            sd_accuracy = sd(correct_answer, na.rm = TRUE)/sqrt(length(correct_answer))) 
part_mean_train_acc_round <- part_mean_train_acc
part_mean_train_acc_round$acc <- round(part_mean_train_acc_round$acc, digits = 2)

ggplot(data = part_mean_train_acc_round) +
  geom_bar(mapping = aes(x = acc)) +
  facet_wrap(~ session) +
  labs(x = "Mean accuracy", y = "Nº of participants")
```
```{r}
#filter participants according to accuracy
participants_high_acc <- subset(part_mean_train_acc,  acc > 0.6, pNum, drop = TRUE)
high_acc_training <- filter(training, pNum %in% participants_high_acc)
high_acc_test1 <- filter(test1, pNum %in% participants_high_acc)
high_acc_test2 <- filter(test2, pNum %in% participants_high_acc)
participants_low_acc <- subset(part_mean_train_acc,  acc <= 0.6, pNum, drop = TRUE)
low_acc_training <- filter(training, pNum %in% participants_low_acc)
low_acc_test1 <- filter(test1, pNum %in% participants_low_acc)
low_acc_test2 <- filter(test2, pNum %in% participants_low_acc)
```


#High accuracy
##Test1
### Accuracy
```{r}
#plot test1 accuracy
m_acc_high_acc_test1 <- high_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_acc)) +
  geom_errorbar(aes(x = cue_type, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for each type of cue in test1 phase")
```

```{r}
#ANOVA accuracy
acc_high_acc_test1 <- high_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_high_acc_test1$predictiveness <- factor(acc_high_acc_test1$predictiveness)
acc_high_acc_test1$session <- factor(acc_high_acc_test1$session)
acc_high_acc_test1$pNum <- factor(acc_high_acc_test1$pNum)
ANOVA_acc_high_acc_test1 <- aov_car(formula = acc ~ session + Error(pNum*predictiveness), data = acc_high_acc_test1)
print(ANOVA_acc_high_acc_test1)
```

```{r}
bay_ANOVA_acc_high_acc_test1 <- anovaBF(formula = acc ~ session*predictiveness + pNum,
        data = data.frame(acc_high_acc_test1),
        whichRandom = "pNum")
print(bay_ANOVA_acc_high_acc_test1)
```

```{r}
bay_ANOVA_acc_high_acc_test1[4]/bay_ANOVA_acc_high_acc_test1[3]
```
Except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the predictive targets. However, there are no significant differences and the bayesian evidence is mild.

###Memory score
```{r}
#plot test mem_score
m_mem_high_acc_test1 <- high_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for each type of cue in test1 phase")
```

```{r}
#ANOVA mem_score
mem_score_high_acc_test1 <- high_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(memory_score = mean(mem_score, na.rm = TRUE))
mem_score_high_acc_test1$predictiveness <- factor(mem_score_high_acc_test1$predictiveness)
mem_score_high_acc_test1$session <- factor(mem_score_high_acc_test1$session)
mem_score_high_acc_test1$pNum <- factor(mem_score_high_acc_test1$pNum)
ANOVA_mem_score_high_acc_test1 <- aov_car(formula = memory_score ~ session + Error(pNum*predictiveness), data = mem_score_high_acc_test1)
print(ANOVA_mem_score_high_acc_test1)
```

```{r}
bay_ANOVA_mem_score_high_acc_test1 <- anovaBF(formula = memory_score ~ session*predictiveness + pNum,
        data = data.frame(mem_score_high_acc_test1),
        whichRandom = "pNum")
print(bay_ANOVA_mem_score_high_acc_test1)
```

```{r}
bay_ANOVA_mem_score_high_acc_test1[4]/bay_ANOVA_mem_score_high_acc_test1[3]
```
Again, except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the precitive targets. However, there are no significant differences and the bayesian evidence is mild.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_high_acc_test1 <- filter(high_acc_test1, acc == 1)
c_m_mem_high_acc_test1 <- c_high_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score for each type of cue in test1 phase")
```
```{r}
#ANOVA mem_score
c_mem_score_high_acc_test1 <- c_high_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
c_mem_score_high_acc_test1$predictiveness <- factor(c_mem_score_high_acc_test1$predictiveness)
c_mem_score_high_acc_test1$session <- factor(c_mem_score_high_acc_test1$session)
c_mem_score_high_acc_test1$pNum <- factor(c_mem_score_high_acc_test1$pNum)
c_ANOVA_mem_score_high_acc_test1 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = c_mem_score_high_acc_test1)
print(c_ANOVA_mem_score_high_acc_test1)
```

```{r}
c_bay_ANOVA_mem_score_high_acc_test1 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
        data = data.frame(c_mem_score_high_acc_test1),
        whichRandom = "pNum")
print(c_bay_ANOVA_mem_score_high_acc_test1)
```

```{r}
c_bay_ANOVA_mem_score_high_acc_test1[4]/c_bay_ANOVA_mem_score_high_acc_test1[3]
```
When memory score is corrected in test 1, there is a significant effect of predictiveness, but the bayesian evidence is anecdotal.

##Test2
### Accuracy
```{r}
#plot test accuracy
m_acc_high_acc_test2 <- high_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_acc)) +
  geom_errorbar(aes(x = cue_type, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for each type of cue in test2 phase")
```

```{r}
#ANOVA accuracy
acc_high_acc_test2 <- high_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_high_acc_test2$predictiveness <- factor(acc_high_acc_test2$predictiveness)
acc_high_acc_test2$session <- factor(acc_high_acc_test2$session)
acc_high_acc_test2$pNum <- factor(acc_high_acc_test2$pNum)
ANOVA_acc_high_acc_test2 <- aov_car(formula = acc ~ session + Error(pNum*predictiveness), data = acc_high_acc_test2)
print(ANOVA_acc_high_acc_test2)
```

```{r}
bay_ANOVA_acc_high_acc_test2 <- anovaBF(formula = acc ~ session*predictiveness + pNum,
        data = data.frame(acc_high_acc_test2),
        whichRandom = "pNum")
print(bay_ANOVA_acc_high_acc_test2)
```

```{r}
bay_ANOVA_acc_high_acc_test2[4]/bay_ANOVA_acc_high_acc_test2[3]
```
There are no differences in accuracy in the second test, confirmed by the ANOVA but with mild bayesian evidence.

### Memory Score

```{r}
#plot test mem_score
m_mem_high_acc_test2 <- high_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for each type of cue in test2 phase")
```

```{r}
#ANOVA mem_score
mem_score_high_acc_test2 <- high_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
mem_score_high_acc_test2$predictiveness <- factor(mem_score_high_acc_test2$predictiveness)
mem_score_high_acc_test2$session <- factor(mem_score_high_acc_test2$session)
mem_score_high_acc_test2$pNum <- factor(mem_score_high_acc_test2$pNum)
ANOVA_mem_score_high_acc_test2 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = mem_score_high_acc_test2)
print(ANOVA_mem_score_high_acc_test2)
```

```{r}
bay_ANOVA_mem_score_high_acc_test2 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
        data = data.frame(mem_score_high_acc_test2),
        whichRandom = "pNum")
print(bay_ANOVA_mem_score_high_acc_test2)
```

```{r}
bay_ANOVA_mem_score_high_acc_test2[4]/bay_ANOVA_mem_score_high_acc_test2[3]
```
In test two, the memory score is always lower for the non-predicitive targets, and the difference is bigger the more difficult the test is. There are significant differences in predictiveness, but the bayesian evidence is anecdotal.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_high_acc_test2 <- filter(high_acc_test2, acc == 1)
c_m_mem_high_acc_test2 <- c_high_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean memory score for each type of cue in test2 phase")
```
```{r}
#ANOVA mem_score
c_mem_score_high_acc_test2 <- c_high_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
c_mem_score_high_acc_test2$predictiveness <- factor(c_mem_score_high_acc_test2$predictiveness)
c_mem_score_high_acc_test2$session <- factor(c_mem_score_high_acc_test2$session)
c_mem_score_high_acc_test2$pNum <- factor(c_mem_score_high_acc_test2$pNum)
c_ANOVA_mem_score_high_acc_test2 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = c_mem_score_high_acc_test2)
print(c_ANOVA_mem_score_high_acc_test2)
```

```{r}
c_bay_ANOVA_mem_score_high_acc_test2 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
        data = data.frame(c_mem_score_high_acc_test2),
        whichRandom = "pNum")
print(c_bay_ANOVA_mem_score_high_acc_test2)
```

```{r}
c_bay_ANOVA_mem_score_high_acc_test2[4]/c_bay_ANOVA_mem_score_high_acc_test2[3]
```
In this case, there´s a clear effect of predictiveness, being the corrected memory score always lower in the non-predicitve targets.

# Very subtle test
## Test1
### Accuracy
```{r}
high_acc_test1_s1 <- filter(high_acc_test1, session == 1)
high_acc_test2_s1 <- filter(high_acc_test2, session == 1)
#plot test accuracy
m_acc_high_acc_test1_s1 <- high_acc_test1_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test1_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for very subtle test")
```

```{r}
#t test accuracy
acc_high_acc_test1_s1 <- high_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test1_s1, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_acc_high_acc_test1_s1 <- t.test(acc ~ predictiveness, data = acc_high_acc_test1_s1, paired = TRUE)
print(t.test_acc_high_acc_test1_s1)
```

```{r}
pred_acc_high_acc_test1_s1 <- subset(acc_high_acc_test1_s1,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test1_s1 <- subset(acc_high_acc_test1_s1,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test1_s1 <-  ttestBF(pred_acc_high_acc_test1_s1, nonpred_acc_high_acc_test1_s1, paired = TRUE)
print(bay_t.test_acc_high_acc_test1_s1)
```
Accuracy is higher in the non-predicitve, but is not significant and the bayesian evidence is not conclusive.

###Memory score
```{r}
#plot test mem_score
m_mem_high_acc_test1_s1 <- high_acc_test1_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test1_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for very subtle test")
```

```{r}
#t test mem_score
mem_score_high_acc_test1_s1 <- high_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test1_s1, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_mem_score_high_acc_test1_s1 <- t.test(mem_score ~ predictiveness, data = mem_score_high_acc_test1_s1, paired = TRUE)
print(t.test_mem_score_high_acc_test1_s1)
```

```{r}
pred_mem_score_high_acc_test1_s1 <- subset(mem_score_high_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test1_s1 <- subset(mem_score_high_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test1_s1 <-  ttestBF(pred_mem_score_high_acc_test1_s1, nonpred_mem_score_high_acc_test1_s1, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test1_s1)
```

Memory score is higher in the non-predicitve, but is not significant and the bayesian evidence is not conclusive.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_high_acc_test1_s1 <- filter(high_acc_test1_s1, acc == 1)
c_m_mem_high_acc_test1_s1 <- c_high_acc_test1_s1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test1_s1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```


```{r}
#t test mem_score
c_mem_score_high_acc_test1_s1 <- c_high_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test1_s1 <- c_mem_score_high_acc_test1_s1[c_mem_score_high_acc_test1_s1$pNum %in% p_pred_c_mem_score_high_acc_test1_s1,]
c_mem_score_high_acc_test1_s1 <- c_mem_score_high_acc_test1_s1[c_mem_score_high_acc_test1_s1$pNum %in% p_nonpred_c_mem_score_high_acc_test1_s1,]
# compute the difference
d <- with(c_mem_score_high_acc_test1_s1, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_high_acc_test1_s1 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test1_s1, paired = TRUE)
print(t.test_c_mem_score_high_acc_test1_s1)
```

```{r}
pred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_high_acc_test1_s1 <-  ttestBF(pred_c_mem_score_high_acc_test1_s1, nonpred_c_mem_score_high_acc_test1_s1, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test1_s1)
```

There are no significant differences in test 1 very subtle when the memory score is corrected.

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_high_acc_test2_s1 <- high_acc_test2_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test2_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in high_acc_test2 phase for very subtle test")
```
```{r}
#t test accuracy
acc_high_acc_test2_s1 <- high_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test2_s1, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
t.test_acc_high_acc_test2_s1 <- t.test(acc ~ predictiveness, data = acc_high_acc_test2_s1, paired = TRUE)
print(t.test_acc_high_acc_test2_s1)
```

```{r}
pred_acc_high_acc_test2_s1 <- subset(acc_high_acc_test2_s1,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test2_s1 <- subset(acc_high_acc_test2_s1,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test2_s1 <-  ttestBF(pred_acc_high_acc_test2_s1, nonpred_acc_high_acc_test2_s1, paired = TRUE)
print(bay_t.test_acc_high_acc_test2_s1)
```

In this case, accuracy is lower for the non-predictive targets, but the difference is not significant and the bayesian evidence is very mild.

###Memory score

```{r}
#plot test mem_score
m_mem_high_acc_test2_s1 <- high_acc_test2_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test2_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in high_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
mem_score_high_acc_test2_s1 <- high_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test2_s1, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
t.test_mem_score_high_acc_test2_s1 <- t.test(mem_score ~ predictiveness, data = mem_score_high_acc_test2_s1, paired = TRUE)
print(t.test_mem_score_high_acc_test2_s1)
```

```{r}
pred_mem_score_high_acc_test2_s1 <- subset(mem_score_high_acc_test2_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test2_s1 <- subset(mem_score_high_acc_test2_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test2_s1 <-  ttestBF(pred_mem_score_high_acc_test2_s1, nonpred_mem_score_high_acc_test2_s1, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test2_s1)
```


Again, memory score is lower in the non-predicitive group. In this case, there are significant differences (p = .035) and anecdotal positive bayesian evidence.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_high_acc_test2_s1 <- filter(high_acc_test2_s1, acc == 1)
c_m_mem_high_acc_test2_s1 <- c_high_acc_test2_s1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test2_s1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean corrected memory score in high_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
c_mem_score_high_acc_test2_s1 <- c_high_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test2_s1 <- subset(c_mem_score_high_acc_test2_s1,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_high_acc_test2_s1 <- subset(c_mem_score_high_acc_test2_s1,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test2_s1 <- c_mem_score_high_acc_test2_s1[c_mem_score_high_acc_test2_s1$pNum %in% p_pred_c_mem_score_high_acc_test2_s1,]
c_mem_score_high_acc_test2_s1 <- c_mem_score_high_acc_test2_s1[c_mem_score_high_acc_test2_s1$pNum %in% p_nonpred_c_mem_score_high_acc_test2_s1,]
# compute the difference
d <- with(c_mem_score_high_acc_test2_s1, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_high_acc_test2_s1 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test2_s1, paired = TRUE)
print(t.test_c_mem_score_high_acc_test2_s1)
```

```{r}
pred_c_mem_score_high_acc_test2_s1 <- subset(c_mem_score_high_acc_test2_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test2_s1 <- subset(c_mem_score_high_acc_test2_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
###can´t find the way of doing this because i can´t install_github("joereinhardt/BayesianFirstAid-Wilcoxon")
#bay_wilcox_c_mem_score_high_acc_test2_s1 <-  bayes.wilcox.test(pred_c_mem_score_high_acc_test2_s1, nonpred_c_mem_score_high_acc_test2_s1, paired = TRUE)
#print
bay_t.test_c_mem_score_high_acc_test2_s1 <-  ttestBF(pred_c_mem_score_high_acc_test2_s1, nonpred_c_mem_score_high_acc_test2_s1, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test2_s1)
```
In this case, there´s a clear effect of predictiveness, being the corrected memory score lower in the non-predicitve targets.

# Subtle test
## Test1
### Accuracy
```{r}
high_acc_test1_s2 <- filter(high_acc_test1, session == 2)
high_acc_test2_s2 <- filter(high_acc_test2, session == 2)
#plot test accuracy
m_acc_high_acc_test1_s2 <- high_acc_test1_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test1_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for subtle test")
```

```{r}
#t test accuracy
acc_high_acc_test1_s2 <- high_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test1_s2, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_acc_high_acc_test1_s2 <- wilcox.test(acc ~ predictiveness, data = acc_high_acc_test1_s2, paired = TRUE)
print(wilcox_acc_high_acc_test1_s2)
```
```{r}
pred_acc_high_acc_test1_s2 <- subset(acc_high_acc_test1_s2,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test1_s2 <- subset(acc_high_acc_test1_s2,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test1_s2 <-  ttestBF(pred_acc_high_acc_test1_s2, nonpred_acc_high_acc_test1_s2, paired = TRUE)
print(bay_t.test_acc_high_acc_test1_s2)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score
```{r}
#plot test mem_score
m_mem_high_acc_test1_s2 <- high_acc_test1_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test1_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for subtle test")
```
```{r}
#t test mem_score
mem_score_high_acc_test1_s2 <- high_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test1_s2, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_mem_score_high_acc_test1_s2 <- t.test(mem_score ~ predictiveness, data = mem_score_high_acc_test1_s2, paired = TRUE)
print(t.test_mem_score_high_acc_test1_s2)
```
```{r}
pred_mem_score_high_acc_test1_s2 <- subset(mem_score_high_acc_test1_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test1_s2 <- subset(mem_score_high_acc_test1_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test1_s2 <-  ttestBF(pred_mem_score_high_acc_test1_s2, nonpred_mem_score_high_acc_test1_s2, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test1_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_high_acc_test1_s2 <- filter(high_acc_test1_s2, acc == 1)
c_m_mem_high_acc_test1_s2 <- c_high_acc_test1_s2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test1_s2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```
```{r}
#t test mem_score
c_mem_score_high_acc_test1_s2 <- c_high_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test1_s2 <- subset(c_mem_score_high_acc_test1_s2,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_high_acc_test1_s2 <- subset(c_mem_score_high_acc_test1_s2,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test1_s2 <- c_mem_score_high_acc_test1_s2[c_mem_score_high_acc_test1_s2$pNum %in% p_pred_c_mem_score_high_acc_test1_s2,]
c_mem_score_high_acc_test1_s2 <- c_mem_score_high_acc_test1_s2[c_mem_score_high_acc_test1_s2$pNum %in% p_nonpred_c_mem_score_high_acc_test1_s2,]
# compute the difference
d <- with(c_mem_score_high_acc_test1_s2, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_high_acc_test1_s2 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test1_s2, paired = TRUE)
print(t.test_c_mem_score_high_acc_test1_s2)
```

```{r}
pred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test1_s1 <- subset(c_mem_score_high_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_high_acc_test1_s1 <-  ttestBF(pred_c_mem_score_high_acc_test1_s1, nonpred_c_mem_score_high_acc_test1_s1, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test1_s1)
```
Responding is lower for the non-predictive targets, there is significant differences but the bayesian test indicates anecdotal evidence for the null hypothesis

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_high_acc_test2_s2 <- high_acc_test2_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test2_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in high_acc_test2 phase for subtle test")
```
```{r}
#t test accuracy
acc_high_acc_test2_s2 <- high_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test2_s2, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
wilcox_acc_high_acc_test2_s2 <- wilcox.test(acc ~ predictiveness, data = acc_high_acc_test2_s2, paired = TRUE)
print(wilcox_acc_high_acc_test2_s2)
```

```{r}
pred_acc_high_acc_test2_s2 <- subset(acc_high_acc_test2_s2,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test2_s2 <- subset(acc_high_acc_test2_s2,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test2_s2 <-  ttestBF(pred_acc_high_acc_test2_s2, nonpred_acc_high_acc_test2_s2, paired = TRUE)
print(bay_t.test_acc_high_acc_test2_s2)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score

```{r}
#plot test mem_score
m_mem_high_acc_test2_s2 <- high_acc_test2_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test2_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test2 phase for subtle test")
```
```{r}
#t test mem_score
mem_score_high_acc_test2_s2 <- high_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test2_s2, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
wilcox_mem_score_high_acc_test2_s2 <- wilcox.test(mem_score ~ predictiveness, data = mem_score_high_acc_test2_s2, paired = TRUE)
print(wilcox_mem_score_high_acc_test2_s2)
```

```{r}
pred_mem_score_high_acc_test2_s2 <- subset(mem_score_high_acc_test2_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test2_s2 <- subset(mem_score_high_acc_test2_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test2_s2 <-  ttestBF(pred_mem_score_high_acc_test2_s2, nonpred_mem_score_high_acc_test2_s2, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test2_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_high_acc_test2_s2 <- filter(high_acc_test2_s2, acc == 1)
c_m_mem_high_acc_test2_s2 <- c_high_acc_test2_s2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test2_s2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean corrected memory score in high_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
c_mem_score_high_acc_test2_s2 <- c_high_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test2_s2 <- subset(c_mem_score_high_acc_test2_s2,  predictiveness == "predictive", pNum, drop = TRUE)
p_nonpred_c_mem_score_high_acc_test2_s2 <- subset(c_mem_score_high_acc_test2_s2,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test2_s2 <- c_mem_score_high_acc_test2_s2[c_mem_score_high_acc_test2_s2$pNum %in% p_pred_c_mem_score_high_acc_test2_s2,]
c_mem_score_high_acc_test2_s2 <- c_mem_score_high_acc_test2_s2[c_mem_score_high_acc_test2_s2$pNum %in% p_nonpred_c_mem_score_high_acc_test2_s2,]
# compute the difference
d <- with(c_mem_score_high_acc_test2_s2, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_high_acc_test2_s2 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test2_s2, paired = TRUE)
print(t.test_c_mem_score_high_acc_test2_s2)
```

```{r}
pred_c_mem_score_high_acc_test2_s2 <- subset(c_mem_score_high_acc_test2_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test2_s2 <- subset(c_mem_score_high_acc_test2_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_high_acc_test2_s2 <-  ttestBF(pred_c_mem_score_high_acc_test2_s2, nonpred_c_mem_score_high_acc_test2_s2, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test2_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

# No subtle test
## Test1
### Accuracy
```{r}
high_acc_test1_s3 <- filter(high_acc_test1, session == 3)
high_acc_test2_s3 <- filter(high_acc_test2, session == 3)
#plot test accuracy
m_acc_high_acc_test1_s3 <- high_acc_test1_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test1_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for no subtle test")
```
```{r}
#t test accuracy
acc_high_acc_test1_s3 <- high_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test1_s3, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_acc_high_acc_test1_s3 <- wilcox.test(acc ~ predictiveness, data = acc_high_acc_test1_s3, paired = TRUE)
print(wilcox_acc_high_acc_test1_s3)
```
```{r}
pred_acc_high_acc_test1_s3 <- subset(acc_high_acc_test1_s3,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test1_s3 <- subset(acc_high_acc_test1_s3,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test1_s3 <-  ttestBF(pred_acc_high_acc_test1_s3, nonpred_acc_high_acc_test1_s3, paired = TRUE)
print(bay_t.test_acc_high_acc_test1_s3)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score
```{r}
#plot test mem_score
m_mem_high_acc_test1_s3 <- high_acc_test1_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test1_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for no subtle test")
```
```{r}
#t test mem_score
mem_score_high_acc_test1_s3 <- high_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test1_s3, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_mem_score_high_acc_test1_s3 <- wilcox.test(mem_score ~ predictiveness, data = mem_score_high_acc_test1_s3, paired = TRUE)
print(wilcox_mem_score_high_acc_test1_s3)
```
```{r}
pred_mem_score_high_acc_test1_s3 <- subset(mem_score_high_acc_test1_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test1_s3 <- subset(mem_score_high_acc_test1_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test1_s3 <-  ttestBF(pred_mem_score_high_acc_test1_s3, nonpred_mem_score_high_acc_test1_s3, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test1_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_high_acc_test1_s3 <- filter(high_acc_test1_s3, acc == 1)
c_m_mem_high_acc_test1_s3 <- c_high_acc_test1_s3 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test1_s3) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```

```{r}
#t test mem_score
c_mem_score_high_acc_test1_s3 <- c_high_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test1_s3 <- subset(c_mem_score_high_acc_test1_s3,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_high_acc_test1_s3 <- subset(c_mem_score_high_acc_test1_s3,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test1_s3 <- c_mem_score_high_acc_test1_s3[c_mem_score_high_acc_test1_s3$pNum %in% p_pred_c_mem_score_high_acc_test1_s3,]
c_mem_score_high_acc_test1_s3 <- c_mem_score_high_acc_test1_s3[c_mem_score_high_acc_test1_s3$pNum %in% p_nonpred_c_mem_score_high_acc_test1_s3,]
# compute the difference
d <- with(c_mem_score_high_acc_test1_s3, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_high_acc_test1_s3 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test1_s3, paired = TRUE)
print(t.test_c_mem_score_high_acc_test1_s3)
```

```{r}
pred_c_mem_score_high_acc_test1_s3 <- subset(c_mem_score_high_acc_test1_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test1_s3 <- subset(c_mem_score_high_acc_test1_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_high_acc_test1_s3 <-  ttestBF(pred_c_mem_score_high_acc_test1_s3, nonpred_c_mem_score_high_acc_test1_s3, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test1_s3)
```
There are no significant differences in responding.

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_high_acc_test2_s3 <- high_acc_test2_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_high_acc_test2_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for no subtle group in high_acc_test2 phase")
```

```{r}
#t test accuracy
acc_high_acc_test2_s3 <- high_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_high_acc_test2_s3, 
        acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
t.test_acc_high_acc_test2_s3 <- t.test(acc ~ predictiveness, data = acc_high_acc_test2_s3, paired = TRUE)
print(t.test_acc_high_acc_test2_s3)
```

```{r}
pred_acc_high_acc_test2_s3 <- subset(acc_high_acc_test2_s3,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_high_acc_test2_s3 <- subset(acc_high_acc_test2_s3,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_high_acc_test2_s3 <-  ttestBF(pred_acc_high_acc_test2_s3, nonpred_acc_high_acc_test2_s3, paired = TRUE)
print(bay_t.test_acc_high_acc_test2_s3)
```
There are no differences in responding depending on the predictiveness of the target.

###Memory score

```{r}
#plot test mem_score
m_mem_high_acc_test2_s3 <- high_acc_test2_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_high_acc_test2_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for group no subtle in test2 phase")
```

```{r}
#t test mem_score
mem_score_high_acc_test2_s3 <- high_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_high_acc_test2_s3, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
t.test_mem_score_high_acc_test2_s3 <- t.test(mem_score ~ predictiveness, data = mem_score_high_acc_test2_s3, paired = TRUE)
print(t.test_mem_score_high_acc_test2_s3)
```

```{r}
pred_mem_score_high_acc_test2_s3 <- subset(mem_score_high_acc_test2_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_high_acc_test2_s3 <- subset(mem_score_high_acc_test2_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_high_acc_test2_s3 <-  ttestBF(pred_mem_score_high_acc_test2_s3, nonpred_mem_score_high_acc_test2_s3, paired = TRUE)
print(bay_t.test_mem_score_high_acc_test2_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot high_acc_test2 mem_score but take out the errors
c_high_acc_test2_s3 <- filter(high_acc_test2_s3, acc == 1)
c_m_mem_high_acc_test2_s3 <- c_high_acc_test2_s3 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_high_acc_test2_s3) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in high_acc_test2 phase for no subtle test")
```

```{r}
#t test mem_score
c_mem_score_high_acc_test2_s3 <- c_high_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_high_acc_test2_s3 <- subset(c_mem_score_high_acc_test2_s3,  predictiveness == "predictive", pNum, drop = TRUE)
p_nonpred_c_mem_score_high_acc_test2_s3 <- subset(c_mem_score_high_acc_test2_s3,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_high_acc_test2_s3 <- c_mem_score_high_acc_test2_s3[c_mem_score_high_acc_test2_s3$pNum %in% p_pred_c_mem_score_high_acc_test2_s3,]
c_mem_score_high_acc_test2_s3 <- c_mem_score_high_acc_test2_s3[c_mem_score_high_acc_test2_s3$pNum %in% p_nonpred_c_mem_score_high_acc_test2_s3,]
# compute the difference
d <- with(c_mem_score_high_acc_test2_s3, 
        mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_high_acc_test2_s3 <- t.test(mem_score ~ predictiveness, data = c_mem_score_high_acc_test2_s3, paired = TRUE)
print(t.test_c_mem_score_high_acc_test2_s3)
```
```{r}
pred_c_mem_score_high_acc_test2_s3 <- subset(c_mem_score_high_acc_test2_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_high_acc_test2_s3 <- subset(c_mem_score_high_acc_test2_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_high_acc_test2_s3 <-  ttestBF(pred_c_mem_score_high_acc_test2_s3, nonpred_c_mem_score_high_acc_test2_s3, paired = TRUE)
print(bay_t.test_c_mem_score_high_acc_test2_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

#Low accuracy
##Test1
### Accuracy
```{r}
#plot test1 accuracy
m_acc_low_acc_test1 <- low_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_acc)) +
  geom_errorbar(aes(x = cue_type, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for each type of cue in test1 phase")
```

```{r}
#ANOVA accuracy
acc_low_acc_test1 <- low_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_low_acc_test1$predictiveness <- factor(acc_low_acc_test1$predictiveness)
acc_low_acc_test1$session <- factor(acc_low_acc_test1$session)
acc_low_acc_test1$pNum <- factor(acc_low_acc_test1$pNum)
ANOVA_acc_low_acc_test1 <- aov_car(formula = acc ~ session + Error(pNum*predictiveness), data = acc_low_acc_test1)
print(ANOVA_acc_low_acc_test1)
```

```{r}
bay_ANOVA_acc_low_acc_test1 <- anovaBF(formula = acc ~ session*predictiveness + pNum,
                                        data = data.frame(acc_low_acc_test1),
                                        whichRandom = "pNum")
print(bay_ANOVA_acc_low_acc_test1)
```

```{r}
bay_ANOVA_acc_low_acc_test1[4]/bay_ANOVA_acc_low_acc_test1[3]
```
Except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the predictive targets. However, there are no significant differences and the bayesian evidence is mild.

###Memory score
```{r}
#plot test mem_score
m_mem_low_acc_test1 <- low_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for each type of cue in test1 phase")
```

```{r}
#ANOVA mem_score
mem_score_low_acc_test1 <- low_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(memory_score = mean(mem_score, na.rm = TRUE))
mem_score_low_acc_test1$predictiveness <- factor(mem_score_low_acc_test1$predictiveness)
mem_score_low_acc_test1$session <- factor(mem_score_low_acc_test1$session)
mem_score_low_acc_test1$pNum <- factor(mem_score_low_acc_test1$pNum)
ANOVA_mem_score_low_acc_test1 <- aov_car(formula = memory_score ~ session + Error(pNum*predictiveness), data = mem_score_low_acc_test1)
print(ANOVA_mem_score_low_acc_test1)
```

```{r}
bay_ANOVA_mem_score_low_acc_test1 <- anovaBF(formula = memory_score ~ session*predictiveness + pNum,
                                              data = data.frame(mem_score_low_acc_test1),
                                              whichRandom = "pNum")
print(bay_ANOVA_mem_score_low_acc_test1)
```

```{r}
bay_ANOVA_mem_score_low_acc_test1[4]/bay_ANOVA_mem_score_low_acc_test1[3]
```
Again, except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the precitive targets. However, there are no significant differences and the bayesian evidence is mild.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_low_acc_test1 <- filter(low_acc_test1, acc == 1)
c_m_mem_low_acc_test1 <- c_low_acc_test1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score for each type of cue in test1 phase")
```
```{r}
#ANOVA mem_score
c_mem_score_low_acc_test1 <- c_low_acc_test1 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
c_mem_score_low_acc_test1$predictiveness <- factor(c_mem_score_low_acc_test1$predictiveness)
c_mem_score_low_acc_test1$session <- factor(c_mem_score_low_acc_test1$session)
c_mem_score_low_acc_test1$pNum <- factor(c_mem_score_low_acc_test1$pNum)
c_ANOVA_mem_score_low_acc_test1 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = c_mem_score_low_acc_test1)
print(c_ANOVA_mem_score_low_acc_test1)
```

```{r}
c_bay_ANOVA_mem_score_low_acc_test1 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
                                                data = data.frame(c_mem_score_low_acc_test1),
                                                whichRandom = "pNum")
print(c_bay_ANOVA_mem_score_low_acc_test1)
```

```{r}
c_bay_ANOVA_mem_score_low_acc_test1[4]/c_bay_ANOVA_mem_score_low_acc_test1[3]
```
When memory score is corrected in test 1, there is a significant effect of predictiveness, but the bayesian evidence is anecdotal.

##Test2
### Accuracy
```{r}
#plot test accuracy
m_acc_low_acc_test2 <- low_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_acc)) +
  geom_errorbar(aes(x = cue_type, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for each type of cue in test2 phase")
```

```{r}
#ANOVA accuracy
acc_low_acc_test2 <- low_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_low_acc_test2$predictiveness <- factor(acc_low_acc_test2$predictiveness)
acc_low_acc_test2$session <- factor(acc_low_acc_test2$session)
acc_low_acc_test2$pNum <- factor(acc_low_acc_test2$pNum)
ANOVA_acc_low_acc_test2 <- aov_car(formula = acc ~ session + Error(pNum*predictiveness), data = acc_low_acc_test2)
print(ANOVA_acc_low_acc_test2)
```

```{r}
bay_ANOVA_acc_low_acc_test2 <- anovaBF(formula = acc ~ session*predictiveness + pNum,
                                        data = data.frame(acc_low_acc_test2),
                                        whichRandom = "pNum")
print(bay_ANOVA_acc_low_acc_test2)
```

```{r}
bay_ANOVA_acc_low_acc_test2[4]/bay_ANOVA_acc_low_acc_test2[3]
```
There are no differences in accuracy in the second test, confirmed by the ANOVA but with mild bayesian evidence.

### Memory Score

```{r}
#plot test mem_score
m_mem_low_acc_test2 <- low_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for each type of cue in test2 phase")
```

```{r}
#ANOVA mem_score
mem_score_low_acc_test2 <- low_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
mem_score_low_acc_test2$predictiveness <- factor(mem_score_low_acc_test2$predictiveness)
mem_score_low_acc_test2$session <- factor(mem_score_low_acc_test2$session)
mem_score_low_acc_test2$pNum <- factor(mem_score_low_acc_test2$pNum)
ANOVA_mem_score_low_acc_test2 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = mem_score_low_acc_test2)
print(ANOVA_mem_score_low_acc_test2)
```

```{r}
bay_ANOVA_mem_score_low_acc_test2 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
                                              data = data.frame(mem_score_low_acc_test2),
                                              whichRandom = "pNum")
print(bay_ANOVA_mem_score_low_acc_test2)
```

```{r}
bay_ANOVA_mem_score_low_acc_test2[4]/bay_ANOVA_mem_score_low_acc_test2[3]
```
In test two, the memory score is always lower for the non-predicitive targets, and the difference is bigger the more difficult the test is. There are significant differences in predictiveness, but the bayesian evidence is anecdotal.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_low_acc_test2 <- filter(low_acc_test2, acc == 1)
c_m_mem_low_acc_test2 <- c_low_acc_test2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean memory score for each type of cue in test2 phase")
```
```{r}
#ANOVA mem_score
c_mem_score_low_acc_test2 <- c_low_acc_test2 %>%
  group_by (pNum, session, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
c_mem_score_low_acc_test2$predictiveness <- factor(c_mem_score_low_acc_test2$predictiveness)
c_mem_score_low_acc_test2$session <- factor(c_mem_score_low_acc_test2$session)
c_mem_score_low_acc_test2$pNum <- factor(c_mem_score_low_acc_test2$pNum)
c_ANOVA_mem_score_low_acc_test2 <- aov_car(formula = mem_score ~ session + Error(pNum*predictiveness), data = c_mem_score_low_acc_test2)
print(c_ANOVA_mem_score_low_acc_test2)
```

```{r}
c_bay_ANOVA_mem_score_low_acc_test2 <- anovaBF(formula = mem_score ~ session*predictiveness + pNum,
                                                data = data.frame(c_mem_score_low_acc_test2),
                                                whichRandom = "pNum")
print(c_bay_ANOVA_mem_score_low_acc_test2)
```

```{r}
c_bay_ANOVA_mem_score_low_acc_test2[4]/c_bay_ANOVA_mem_score_low_acc_test2[3]
```
In this case, there´s a clear effect of predictiveness, being the corrected memory score always lower in the non-predicitve targets.

# Very subtle test
## Test1
### Accuracy
```{r}
low_acc_test1_s1 <- filter(low_acc_test1, session == 1)
low_acc_test2_s1 <- filter(low_acc_test2, session == 1)
#plot test accuracy
m_acc_low_acc_test1_s1 <- low_acc_test1_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test1_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for very subtle test")
```

```{r}
#t test accuracy
acc_low_acc_test1_s1 <- low_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test1_s1, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_acc_low_acc_test1_s1 <- t.test(acc ~ predictiveness, data = acc_low_acc_test1_s1, paired = TRUE)
print(t.test_acc_low_acc_test1_s1)
```

```{r}
pred_acc_low_acc_test1_s1 <- subset(acc_low_acc_test1_s1,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test1_s1 <- subset(acc_low_acc_test1_s1,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test1_s1 <-  ttestBF(pred_acc_low_acc_test1_s1, nonpred_acc_low_acc_test1_s1, paired = TRUE)
print(bay_t.test_acc_low_acc_test1_s1)
```
Accuracy is lower in the non-predicitve, but is not significant and the bayesian evidence is not conclusive.

###Memory score
```{r}
#plot test mem_score
m_mem_low_acc_test1_s1 <- low_acc_test1_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test1_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for very subtle test")
```

```{r}
#t test mem_score
mem_score_low_acc_test1_s1 <- low_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test1_s1, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_mem_score_low_acc_test1_s1 <- t.test(mem_score ~ predictiveness, data = mem_score_low_acc_test1_s1, paired = TRUE)
print(t.test_mem_score_low_acc_test1_s1)
```

```{r}
pred_mem_score_low_acc_test1_s1 <- subset(mem_score_low_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test1_s1 <- subset(mem_score_low_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test1_s1 <-  ttestBF(pred_mem_score_low_acc_test1_s1, nonpred_mem_score_low_acc_test1_s1, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test1_s1)
```

Memory score is lower in the non-predicitve, but is not significant and the bayesian evidence is not conclusive.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_low_acc_test1_s1 <- filter(low_acc_test1_s1, acc == 1)
c_m_mem_low_acc_test1_s1 <- c_low_acc_test1_s1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test1_s1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```


```{r}
#t test mem_score
c_mem_score_low_acc_test1_s1 <- c_low_acc_test1_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test1_s1 <- c_mem_score_low_acc_test1_s1[c_mem_score_low_acc_test1_s1$pNum %in% p_pred_c_mem_score_low_acc_test1_s1,]
c_mem_score_low_acc_test1_s1 <- c_mem_score_low_acc_test1_s1[c_mem_score_low_acc_test1_s1$pNum %in% p_nonpred_c_mem_score_low_acc_test1_s1,]
# compute the difference
d <- with(c_mem_score_low_acc_test1_s1, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_low_acc_test1_s1 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test1_s1, paired = TRUE)
print(t.test_c_mem_score_low_acc_test1_s1)
```

```{r}
pred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_low_acc_test1_s1 <-  ttestBF(pred_c_mem_score_low_acc_test1_s1, nonpred_c_mem_score_low_acc_test1_s1, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test1_s1)
```

There are no significant differences in test 1 very subtle when the memory score is corrected.

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_low_acc_test2_s1 <- low_acc_test2_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test2_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in low_acc_test2 phase for very subtle test")
```
```{r}
#t test accuracy
acc_low_acc_test2_s1 <- low_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test2_s1, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
wilcox_acc_low_acc_test2_s1 <- wilcox.test(acc ~ predictiveness, data = acc_low_acc_test2_s1, paired = TRUE)
print(wilcox_acc_low_acc_test2_s1)
```

```{r}
pred_acc_low_acc_test2_s1 <- subset(acc_low_acc_test2_s1,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test2_s1 <- subset(acc_low_acc_test2_s1,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test2_s1 <-  ttestBF(pred_acc_low_acc_test2_s1, nonpred_acc_low_acc_test2_s1, paired = TRUE)
print(bay_t.test_acc_low_acc_test2_s1)
```

In this case, accuracy is lower for the non-predictive targets, but the difference is not significant and the bayesian evidence is very mild.

###Memory score

```{r}
#plot test mem_score
m_mem_low_acc_test2_s1 <- low_acc_test2_s1 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test2_s1) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in low_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
mem_score_low_acc_test2_s1 <- low_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test2_s1, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
wilcox_mem_score_low_acc_test2_s1 <- wilcox.test(mem_score ~ predictiveness, data = mem_score_low_acc_test2_s1, paired = TRUE)
print(wilcox_mem_score_low_acc_test2_s1)
```

```{r}
pred_mem_score_low_acc_test2_s1 <- subset(mem_score_low_acc_test2_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test2_s1 <- subset(mem_score_low_acc_test2_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test2_s1 <-  ttestBF(pred_mem_score_low_acc_test2_s1, nonpred_mem_score_low_acc_test2_s1, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test2_s1)
```


Again, memory score is lower in the non-predicitive group. In this case, there are significant differences (p = .035) and anecdotal positive bayesian evidence.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_low_acc_test2_s1 <- filter(low_acc_test2_s1, acc == 1)
c_m_mem_low_acc_test2_s1 <- c_low_acc_test2_s1 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test2_s1) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean corrected memory score in low_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
c_mem_score_low_acc_test2_s1 <- c_low_acc_test2_s1 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test2_s1 <- subset(c_mem_score_low_acc_test2_s1,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_low_acc_test2_s1 <- subset(c_mem_score_low_acc_test2_s1,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test2_s1 <- c_mem_score_low_acc_test2_s1[c_mem_score_low_acc_test2_s1$pNum %in% p_pred_c_mem_score_low_acc_test2_s1,]
c_mem_score_low_acc_test2_s1 <- c_mem_score_low_acc_test2_s1[c_mem_score_low_acc_test2_s1$pNum %in% p_nonpred_c_mem_score_low_acc_test2_s1,]
# compute the difference
d <- with(c_mem_score_low_acc_test2_s1, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_low_acc_test2_s1 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test2_s1, paired = TRUE)
print(t.test_c_mem_score_low_acc_test2_s1)
```

```{r}
pred_c_mem_score_low_acc_test2_s1 <- subset(c_mem_score_low_acc_test2_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test2_s1 <- subset(c_mem_score_low_acc_test2_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
###can´t find the way of doing this because i can´t install_github("joereinhardt/BayesianFirstAid-Wilcoxon")
#bay_wilcox_c_mem_score_low_acc_test2_s1 <-  bayes.wilcox.test(pred_c_mem_score_low_acc_test2_s1, nonpred_c_mem_score_low_acc_test2_s1, paired = TRUE)
#print
bay_t.test_c_mem_score_low_acc_test2_s1 <-  ttestBF(pred_c_mem_score_low_acc_test2_s1, nonpred_c_mem_score_low_acc_test2_s1, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test2_s1)
```
In this case, there´s a clear effect of predictiveness, being the corrected memory score lower in the non-predicitve targets.

# Subtle test
## Test1
### Accuracy
```{r}
low_acc_test1_s2 <- filter(low_acc_test1, session == 2)
low_acc_test2_s2 <- filter(low_acc_test2, session == 2)
#plot test accuracy
m_acc_low_acc_test1_s2 <- low_acc_test1_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test1_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for subtle test")
```

```{r}
#t test accuracy
acc_low_acc_test1_s2 <- low_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test1_s2, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_acc_low_acc_test1_s2 <- wilcox.test(acc ~ predictiveness, data = acc_low_acc_test1_s2, paired = TRUE)
print(wilcox_acc_low_acc_test1_s2)
```
```{r}
pred_acc_low_acc_test1_s2 <- subset(acc_low_acc_test1_s2,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test1_s2 <- subset(acc_low_acc_test1_s2,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test1_s2 <-  ttestBF(pred_acc_low_acc_test1_s2, nonpred_acc_low_acc_test1_s2, paired = TRUE)
print(bay_t.test_acc_low_acc_test1_s2)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score
```{r}
#plot test mem_score
m_mem_low_acc_test1_s2 <- low_acc_test1_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test1_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for subtle test")
```
```{r}
#t test mem_score
mem_score_low_acc_test1_s2 <- low_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test1_s2, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_mem_score_low_acc_test1_s2 <- t.test(mem_score ~ predictiveness, data = mem_score_low_acc_test1_s2, paired = TRUE)
print(t.test_mem_score_low_acc_test1_s2)
```
```{r}
pred_mem_score_low_acc_test1_s2 <- subset(mem_score_low_acc_test1_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test1_s2 <- subset(mem_score_low_acc_test1_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test1_s2 <-  ttestBF(pred_mem_score_low_acc_test1_s2, nonpred_mem_score_low_acc_test1_s2, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test1_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_low_acc_test1_s2 <- filter(low_acc_test1_s2, acc == 1)
c_m_mem_low_acc_test1_s2 <- c_low_acc_test1_s2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test1_s2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```
```{r}
#t test mem_score
c_mem_score_low_acc_test1_s2 <- c_low_acc_test1_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test1_s2 <- subset(c_mem_score_low_acc_test1_s2,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_low_acc_test1_s2 <- subset(c_mem_score_low_acc_test1_s2,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test1_s2 <- c_mem_score_low_acc_test1_s2[c_mem_score_low_acc_test1_s2$pNum %in% p_pred_c_mem_score_low_acc_test1_s2,]
c_mem_score_low_acc_test1_s2 <- c_mem_score_low_acc_test1_s2[c_mem_score_low_acc_test1_s2$pNum %in% p_nonpred_c_mem_score_low_acc_test1_s2,]
# compute the difference
d <- with(c_mem_score_low_acc_test1_s2, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_low_acc_test1_s2 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test1_s2, paired = TRUE)
print(t.test_c_mem_score_low_acc_test1_s2)
```

```{r}
pred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test1_s1 <- subset(c_mem_score_low_acc_test1_s1,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_low_acc_test1_s1 <-  ttestBF(pred_c_mem_score_low_acc_test1_s1, nonpred_c_mem_score_low_acc_test1_s1, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test1_s1)
```
Responding is lower for the non-predictive targets, there is significant differences but the bayesian test indicates anecdotal evidence for the null hypothesis

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_low_acc_test2_s2 <- low_acc_test2_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test2_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in low_acc_test2 phase for subtle test")
```
```{r}
#t test accuracy
acc_low_acc_test2_s2 <- low_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test2_s2, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
wilcox_acc_low_acc_test2_s2 <- wilcox.test(acc ~ predictiveness, data = acc_low_acc_test2_s2, paired = TRUE)
print(wilcox_acc_low_acc_test2_s2)
```

```{r}
pred_acc_low_acc_test2_s2 <- subset(acc_low_acc_test2_s2,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test2_s2 <- subset(acc_low_acc_test2_s2,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test2_s2 <-  ttestBF(pred_acc_low_acc_test2_s2, nonpred_acc_low_acc_test2_s2, paired = TRUE)
print(bay_t.test_acc_low_acc_test2_s2)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score

```{r}
#plot test mem_score
m_mem_low_acc_test2_s2 <- low_acc_test2_s2 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test2_s2) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test2 phase for subtle test")
```
```{r}
#t test mem_score
mem_score_low_acc_test2_s2 <- low_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test2_s2, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
wilcox_mem_score_low_acc_test2_s2 <- wilcox.test(mem_score ~ predictiveness, data = mem_score_low_acc_test2_s2, paired = TRUE)
print(wilcox_mem_score_low_acc_test2_s2)
```

```{r}
pred_mem_score_low_acc_test2_s2 <- subset(mem_score_low_acc_test2_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test2_s2 <- subset(mem_score_low_acc_test2_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test2_s2 <-  ttestBF(pred_mem_score_low_acc_test2_s2, nonpred_mem_score_low_acc_test2_s2, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test2_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test2 mem_score but take out the errors
c_low_acc_test2_s2 <- filter(low_acc_test2_s2, acc == 1)
c_m_mem_low_acc_test2_s2 <- c_low_acc_test2_s2 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test2_s2) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Figure 3", subtitle = "Mean corrected memory score in low_acc_test2 phase for very subtle test")
```
```{r}
#t test mem_score
c_mem_score_low_acc_test2_s2 <- c_low_acc_test2_s2 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test2_s2 <- subset(c_mem_score_low_acc_test2_s2,  predictiveness == "predictive", pNum, drop = TRUE)
p_nonpred_c_mem_score_low_acc_test2_s2 <- subset(c_mem_score_low_acc_test2_s2,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test2_s2 <- c_mem_score_low_acc_test2_s2[c_mem_score_low_acc_test2_s2$pNum %in% p_pred_c_mem_score_low_acc_test2_s2,]
c_mem_score_low_acc_test2_s2 <- c_mem_score_low_acc_test2_s2[c_mem_score_low_acc_test2_s2$pNum %in% p_nonpred_c_mem_score_low_acc_test2_s2,]
# compute the difference
d <- with(c_mem_score_low_acc_test2_s2, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_low_acc_test2_s2 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test2_s2, paired = TRUE)
print(t.test_c_mem_score_low_acc_test2_s2)
```

```{r}
pred_c_mem_score_low_acc_test2_s2 <- subset(c_mem_score_low_acc_test2_s2,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test2_s2 <- subset(c_mem_score_low_acc_test2_s2,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_low_acc_test2_s2 <-  ttestBF(pred_c_mem_score_low_acc_test2_s2, nonpred_c_mem_score_low_acc_test2_s2, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test2_s2)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

# No subtle test
## Test1
### Accuracy
```{r}
low_acc_test1_s3 <- filter(low_acc_test1, session == 3)
low_acc_test2_s3 <- filter(low_acc_test2, session == 3)
#plot test accuracy
m_acc_low_acc_test1_s3 <- low_acc_test1_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test1_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Predicitiveness") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy in test1 phase for no subtle test")
```
```{r}
#t test accuracy
acc_low_acc_test1_s3 <- low_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test1_s3, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_acc_low_acc_test1_s3 <- wilcox.test(acc ~ predictiveness, data = acc_low_acc_test1_s3, paired = TRUE)
print(wilcox_acc_low_acc_test1_s3)
```
```{r}
pred_acc_low_acc_test1_s3 <- subset(acc_low_acc_test1_s3,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test1_s3 <- subset(acc_low_acc_test1_s3,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test1_s3 <-  ttestBF(pred_acc_low_acc_test1_s3, nonpred_acc_low_acc_test1_s3, paired = TRUE)
print(bay_t.test_acc_low_acc_test1_s3)
```
There´s lower accuracy for the non-predictive, but there is no significant difference.

###Memory score
```{r}
#plot test mem_score
m_mem_low_acc_test1_s3 <- low_acc_test1_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test1_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score in test1 phase for no subtle test")
```
```{r}
#t test mem_score
mem_score_low_acc_test1_s3 <- low_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test1_s3, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
wilcox_mem_score_low_acc_test1_s3 <- wilcox.test(mem_score ~ predictiveness, data = mem_score_low_acc_test1_s3, paired = TRUE)
print(wilcox_mem_score_low_acc_test1_s3)
```
```{r}
pred_mem_score_low_acc_test1_s3 <- subset(mem_score_low_acc_test1_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test1_s3 <- subset(mem_score_low_acc_test1_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test1_s3 <-  ttestBF(pred_mem_score_low_acc_test1_s3, nonpred_mem_score_low_acc_test1_s3, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test1_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot test mem_score but take out the errors
c_low_acc_test1_s3 <- filter(low_acc_test1_s3, acc == 1)
c_m_mem_low_acc_test1_s3 <- c_low_acc_test1_s3 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test1_s3) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in test1 phase for the very subtle test")
```

```{r}
#t test mem_score
c_mem_score_low_acc_test1_s3 <- c_low_acc_test1_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test1_s3 <- subset(c_mem_score_low_acc_test1_s3,  predictiveness == "predictive", c(pNum), drop = TRUE)
p_nonpred_c_mem_score_low_acc_test1_s3 <- subset(c_mem_score_low_acc_test1_s3,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test1_s3 <- c_mem_score_low_acc_test1_s3[c_mem_score_low_acc_test1_s3$pNum %in% p_pred_c_mem_score_low_acc_test1_s3,]
c_mem_score_low_acc_test1_s3 <- c_mem_score_low_acc_test1_s3[c_mem_score_low_acc_test1_s3$pNum %in% p_nonpred_c_mem_score_low_acc_test1_s3,]
# compute the difference
d <- with(c_mem_score_low_acc_test1_s3, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
```{r}
t.test_c_mem_score_low_acc_test1_s3 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test1_s3, paired = TRUE)
print(t.test_c_mem_score_low_acc_test1_s3)
```

```{r}
pred_c_mem_score_low_acc_test1_s3 <- subset(c_mem_score_low_acc_test1_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test1_s3 <- subset(c_mem_score_low_acc_test1_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_low_acc_test1_s3 <-  ttestBF(pred_c_mem_score_low_acc_test1_s3, nonpred_c_mem_score_low_acc_test1_s3, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test1_s3)
```
There are no significant differences in responding.

##Test2
###Accuracy

```{r}
#plot test accuracy
m_acc_low_acc_test2_s3 <- low_acc_test2_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            sd_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
ggplot(data = m_acc_low_acc_test2_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_acc)) +
  geom_errorbar(aes(x = predictiveness, y= mean_acc, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Accuracy") +
  labs(title = "Mean accuracy for no subtle group in low_acc_test2 phase")
```

```{r}
#t test accuracy
acc_low_acc_test2_s3 <- low_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
# compute the difference
d <- with(acc_low_acc_test2_s3, 
          acc[predictiveness == "non-predictive"] - acc[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # 
```
```{r}
t.test_acc_low_acc_test2_s3 <- t.test(acc ~ predictiveness, data = acc_low_acc_test2_s3, paired = TRUE)
print(t.test_acc_low_acc_test2_s3)
```

```{r}
pred_acc_low_acc_test2_s3 <- subset(acc_low_acc_test2_s3,  predictiveness == "predictive", acc, drop = TRUE)
nonpred_acc_low_acc_test2_s3 <- subset(acc_low_acc_test2_s3,  predictiveness == "non-predictive", acc, drop = TRUE)
bay_t.test_acc_low_acc_test2_s3 <-  ttestBF(pred_acc_low_acc_test2_s3, nonpred_acc_low_acc_test2_s3, paired = TRUE)
print(bay_t.test_acc_low_acc_test2_s3)
```
There are no differences in responding depending on the predictiveness of the target.

###Memory score

```{r}
#plot test mem_score
m_mem_low_acc_test2_s3 <- low_acc_test2_s3 %>%
  group_by(predictiveness) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = m_mem_low_acc_test2_s3) +
  geom_col(mapping = aes(x = predictiveness, y = mean_mem_score)) +
  geom_errorbar(aes(x = predictiveness, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean memory score for group no subtle in test2 phase")
```

```{r}
#t test mem_score
mem_score_low_acc_test2_s3 <- low_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
# compute the difference
d <- with(mem_score_low_acc_test2_s3, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) 
```
```{r}
t.test_mem_score_low_acc_test2_s3 <- t.test(mem_score ~ predictiveness, data = mem_score_low_acc_test2_s3, paired = TRUE)
print(t.test_mem_score_low_acc_test2_s3)
```

```{r}
pred_mem_score_low_acc_test2_s3 <- subset(mem_score_low_acc_test2_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_mem_score_low_acc_test2_s3 <- subset(mem_score_low_acc_test2_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_mem_score_low_acc_test2_s3 <-  ttestBF(pred_mem_score_low_acc_test2_s3, nonpred_mem_score_low_acc_test2_s3, paired = TRUE)
print(bay_t.test_mem_score_low_acc_test2_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.

###Corrected memory score
```{r}
#plot low_acc_test2 mem_score but take out the errors
c_low_acc_test2_s3 <- filter(low_acc_test2_s3, acc == 1)
c_m_mem_low_acc_test2_s3 <- c_low_acc_test2_s3 %>%
  group_by(cue_type) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            sd_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
ggplot(data = c_m_mem_low_acc_test2_s3) +
  geom_col(mapping = aes(x = cue_type, y = mean_mem_score)) +
  geom_errorbar(aes(x = cue_type, y= mean_mem_score, ymin = mean_mem_score - sd_mem_score, ymax = mean_mem_score + sd_mem_score)) +
  coord_cartesian(ylim = c(0, 10))+
  scale_x_discrete (name = "Type of cue") +
  scale_y_continuous(name = "Memory score") +
  labs(title = "Mean corrected memory score in low_acc_test2 phase for no subtle test")
```

```{r}
#t test mem_score
c_mem_score_low_acc_test2_s3 <- c_low_acc_test2_s3 %>%
  group_by (pNum, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
p_pred_c_mem_score_low_acc_test2_s3 <- subset(c_mem_score_low_acc_test2_s3,  predictiveness == "predictive", pNum, drop = TRUE)
p_nonpred_c_mem_score_low_acc_test2_s3 <- subset(c_mem_score_low_acc_test2_s3,  predictiveness == "non-predictive", pNum, drop = TRUE)
c_mem_score_low_acc_test2_s3 <- c_mem_score_low_acc_test2_s3[c_mem_score_low_acc_test2_s3$pNum %in% p_pred_c_mem_score_low_acc_test2_s3,]
c_mem_score_low_acc_test2_s3 <- c_mem_score_low_acc_test2_s3[c_mem_score_low_acc_test2_s3$pNum %in% p_nonpred_c_mem_score_low_acc_test2_s3,]
# compute the difference
d <- with(c_mem_score_low_acc_test2_s3, 
          mem_score[predictiveness == "non-predictive"] - mem_score[predictiveness == "predictive"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d)
```
```{r}
t.test_c_mem_score_low_acc_test2_s3 <- t.test(mem_score ~ predictiveness, data = c_mem_score_low_acc_test2_s3, paired = TRUE)
print(t.test_c_mem_score_low_acc_test2_s3)
```
```{r}
pred_c_mem_score_low_acc_test2_s3 <- subset(c_mem_score_low_acc_test2_s3,  predictiveness == "predictive", mem_score, drop = TRUE)
nonpred_c_mem_score_low_acc_test2_s3 <- subset(c_mem_score_low_acc_test2_s3,  predictiveness == "non-predictive", mem_score, drop = TRUE)
bay_t.test_c_mem_score_low_acc_test2_s3 <-  ttestBF(pred_c_mem_score_low_acc_test2_s3, nonpred_c_mem_score_low_acc_test2_s3, paired = TRUE)
print(bay_t.test_c_mem_score_low_acc_test2_s3)
```
There´s lower memory score for the non-predictive, but there is no significant difference.